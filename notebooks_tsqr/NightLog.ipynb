{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters. \n",
    "# Times Square replaces this cell with the user's parameters.\n",
    "\n",
    "# The run-time defaults for all of these parameters are in NightLog.yaml\n",
    "# Under Times Square, the run-time defaults always override values given here.\n",
    "# Values here are used for local tests.\n",
    "\n",
    "# day_obs values: TODAY, YESTERDAY, YYYY-MM-DD\n",
    "# Report on observing nights that start upto but not included this day.\n",
    "#!day_obs = '2024-09-18' # Value to use for local testing (Summit)\n",
    "day_obs = 'TODAY' # TODO Change to 'TODAY' to test with default before push  \n",
    "\n",
    "# Total number of days of data to display (ending on day_obs)\n",
    "number_of_days = '3'  # TODO Change to '1' to test with default before push  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Only use packages available in the Rubin Science Platform\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from pprint import pp\n",
    "from urllib.parse import urlencode\n",
    "from IPython.display import display, Markdown, display_markdown\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from datetime import datetime, date, timedelta\n",
    "#! from rubin_scheduler.site_models import Almanac\n",
    "\n",
    "# Normalize Parameters (both explicit Times Squares params, in implicit ones)\n",
    "limit = 50  # YAGNI: Auto get more if this isn't enough to get all requested DAYS\n",
    "\n",
    "match day_obs.lower():\n",
    "    case 'today':\n",
    "        date = datetime.now().date()\n",
    "    case 'yesterday':\n",
    "        date = datetime.now().date()-timedelta(days=1)\n",
    "    case _:\n",
    "        date = datetime.strptime(day_obs, '%Y-%m-%d').date()\n",
    "# date:  is EXLUSIVE (upto, but not including)\n",
    "days = int(number_of_days)\n",
    "\n",
    "# Thus: [min_day_obs,max_day_obs)\n",
    "min_day_obs = (date - timedelta(days=days-1)).strftime('%Y%m%d') # Inclusive\n",
    "max_day_obs = (date + timedelta(days=1)).strftime('%Y%m%d') # prep for Exclusive\n",
    "\n",
    "response_timeout = 3.05  # seconds, how long to wait for connection\n",
    "read_timeout = 20  # seconds\n",
    "timeout = (float(response_timeout), float(read_timeout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default env to \"usdf\" and try before PUSH to repo.\n",
    "summit = 'https://summit-lsp.lsst.codes'\n",
    "usdf = 'https://usdf-rsp-dev.slac.stanford.edu'\n",
    "tucson = 'https://tucson-teststand.lsst.codes'\n",
    "\n",
    "# The default provided here is for local testing.\n",
    "# Under Times Square it is ignored.\n",
    "server = os.environ.get('EXTERNAL_INSTANCE_URL', summit) # TODO try with \"usdf\" before push (else \"summit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# When running under Times Square, install pkg from github.\n",
    "# Otherwise use what is installed locally (intended to be dev editiable pkg)\n",
    "if os.environ.get('EXTERNAL_INSTANCE_URL'):\n",
    "    print('Installing \"lsst.ts.logging_and_reporting\" from github using \"prototype\" branch....')\n",
    "    !pip install --upgrade git+https://github.com/lsst-ts/ts_logging_and_reporting.git@prototype >/dev/null\n",
    "import lsst.ts.logging_and_reporting.source_adapters as sad\n",
    "import lsst.ts.logging_and_reporting.almanac as alm\n",
    "import lsst.ts.logging_and_reporting.reports as rep \n",
    "from lsst.ts.logging_and_reporting.reports import md,mdlist, NightlyLogReport\n",
    "\n",
    "try:\n",
    "    import lsst.ts.logging_and_reporting.version\n",
    "    lrversion = lsst.ts.logging_and_reporting.version.__version__\n",
    "except:\n",
    "    lrversion = 'LIVE'\n",
    "\n",
    "try:\n",
    "    from lsst_efd_client import EfdClient\n",
    "    enable_efd = True\n",
    "except:\n",
    "    enable_efd = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "The only environment that has everything needed for this page is\n",
    "https://summit-lsp.lsst.codes\n",
    "\n",
    "However, Times Square **does not** run on the Summit. It **does** run on USDF-dev.  USDF doesn't fully support all the data sources we need so some functionality is currently missing on this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Report for **2024-09-18** covering the previous **3** observing night(s).\n",
       "- Run on logs from **https://summit-lsp.lsst.codes/**\n",
       "- Using *Prototype* Logging and Reporting Version: **0.1.dev63+g83a61e0.d20240918**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "This report will attempt to use the following log sources: \n",
       "- https://summit-lsp.lsst.codes/exposurelog/instruments\n",
       "- https://summit-lsp.lsst.codes/exposurelog/exposures\n",
       "- https://summit-lsp.lsst.codes/exposurelog/messages\n",
       "- https://summit-lsp.lsst.codes/narrativelog/messages\n",
       "- https://summit-lsp.lsst.codes/nightreport/reports"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display overview of Report context \n",
    "md(f'''\n",
    "Report for **{date}** covering the previous **{days}** observing night(s).\n",
    "- Run on logs from **{server}/**\n",
    "- Using *Prototype* Logging and Reporting Version: **{lrversion}**\n",
    "''')\n",
    "\n",
    "ul = '\\n- '.join(['',*sad.all_endpoints(server)])\n",
    "md(f'This report will attempt to use the following log sources: {ul}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Almanac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Moon Rise</th>\n",
       "      <td>2024-09-17 22:25:43.601</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Moon Set</th>\n",
       "      <td>2024-09-17 10:17:44.848</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Moon Illumination</th>\n",
       "      <td>99%</td>\n",
       "      <td>(% lit)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Astronomical Twilight (morning)</th>\n",
       "      <td>2024-09-17 09:18:25.687</td>\n",
       "      <td>(-18 degrees)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Astronomical Twilight (evening)</th>\n",
       "      <td>2024-09-17 23:56:37.079</td>\n",
       "      <td>(-18 degrees)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nautical Twilight (morning)</th>\n",
       "      <td>2024-09-17 09:46:22.573</td>\n",
       "      <td>(-12 degrees)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nautical Twilight (evening)</th>\n",
       "      <td>2024-09-17 23:28:37.050</td>\n",
       "      <td>(-12 degrees)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Civil Twilight (morning)</th>\n",
       "      <td>2024-09-17 10:14:10.623</td>\n",
       "      <td>(-6 degrees)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Civil Twilight (evening)</th>\n",
       "      <td>2024-09-17 23:00:46.504</td>\n",
       "      <td>(-6 degrees)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sun Rise</th>\n",
       "      <td>2024-09-17 10:41:56.478</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sun Set</th>\n",
       "      <td>2024-09-17 22:32:58.645</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0              1\n",
       "Moon Rise                        2024-09-17 22:25:43.601               \n",
       "Moon Set                         2024-09-17 10:17:44.848               \n",
       "Moon Illumination                                    99%        (% lit)\n",
       "Astronomical Twilight (morning)  2024-09-17 09:18:25.687  (-18 degrees)\n",
       "Astronomical Twilight (evening)  2024-09-17 23:56:37.079  (-18 degrees)\n",
       "Nautical Twilight (morning)      2024-09-17 09:46:22.573  (-12 degrees)\n",
       "Nautical Twilight (evening)      2024-09-17 23:28:37.050  (-12 degrees)\n",
       "Civil Twilight (morning)         2024-09-17 10:14:10.623   (-6 degrees)\n",
       "Civil Twilight (evening)         2024-09-17 23:00:46.504   (-6 degrees)\n",
       "Sun Rise                         2024-09-17 10:41:56.478               \n",
       "Sun Set                          2024-09-17 22:32:58.645               "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display various almanac values (for moon, sun)\n",
    "rep.AlmanacReport().almanac_as_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Night Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Nightly Jira BLOCKs\n",
       "- 20240916\n",
       "    - [testPlayer/BLOCK-R89](https://rubinobs.atlassian.net/projects/BLOCK?selectedItem=com.atlassian.plugins.atlassian-connect-plugin:com.kanoah.test-manager__main-project-page#!/testPlayer/BLOCK-R89)\n",
       "    - [v2/testCases](https://rubinobs.atlassian.net/projects/BLOCK?selectedItem=com.atlassian.plugins.atlassian-connect-plugin:com.kanoah.test-manager__main-project-page#!/v2/testCases)\n",
       "- 20240917\n",
       "    - [v2/testCase/BLOCK-T19](https://rubinobs.atlassian.net/projects/BLOCK?selectedItem=com.atlassian.plugins.atlassian-connect-plugin:com.kanoah.test-manager__main-project-page#!/v2/testCase/BLOCK-T19)\n",
       "    - [testPlayer/BLOCK-R90](https://rubinobs.atlassian.net/projects/BLOCK?selectedItem=com.atlassian.plugins.atlassian-connect-plugin:com.kanoah.test-manager__main-project-page#!/testPlayer/BLOCK-R90)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## NIGHT: 20240916: "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### DATE: 2024-09-16: "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:18:03 | <pre>Daytime: Cycle 39 started around 09:00 CLT, so most of CSCs were in Offline or Standby until it was done. \n",
       "\n",
       "We run successfully all the blocks. \n",
       "- BLOCK-T151 (1.0) Simonyi Hexapods Startup. Both M2/Camera Hexapods shows 17.3 C.\n",
       "- BLOCK-T3 (2.0) Warm-up M2 Hexapod.\n",
       "- BLOCK-T4 (2.0) Warm-up Camera Hexapod.\n",
       "- BLOCK-T153 - Opening M1M3 Mirror Covers.\n",
       "- BLOCK-T152  - Closing M1M3 Mirror Covers.\n",
       "- BLOCK-T145 - M1M3 Hardpoints Breakaway Test.\n",
       "- BLOCK-T144 - M1M3 Bump Test - Actuators [238, 330, 409] FAILED the bump test.\n",
       "- BLOCK-T139 - Hexapod Shutdown.\n",
       "\n",
       "The ScriptQueue:1 was updated and patched during the night. After the warm-up, M2 and Camera Hexapod were in disabled state during all the run and we monitor their temperature. Since the end of the warm-up until the hexapod shutdown their temperature increased ~ 1.5 deg C (average).</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:18:03 | <pre>Daytime: Cycle 39 started around 09:00 CLT, so most of CSCs were in Offline or Standby until it was done. \n",
       "\n",
       "We run successfully all the blocks. \n",
       "- BLOCK-T151 (1.0) Simonyi Hexapods Startup. Both M2/Camera Hexapods shows 17.3 C\n",
       "- BLOCK-T3 (2.0) Warm-up M2 Hexapod\n",
       "- BLOCK-T4 (2.0) Warm-up Camera Hexapod\n",
       "- We set DISABLED state for both hexapod and monitor their temperature (temperature increased ~ 1.5 deg C from 18 to 19.4)\n",
       "- BLOCK-T153 - Opening M1M3 Mirror Covers\n",
       "- BLOCK-T152  - Closing M1M3 Mirror Covers \n",
       "- BLOCK-T145 - M1M3 Hardpoints Breakaway Test\n",
       "- Tiago updated ScriptQueue:1\n",
       "- BLOCK-T144 - M1M3 Bump Test - Actuators [238, 330, 409] FAILED the bump test\n",
       "- BLOCK-T139 - Hexapod Shutdown</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:18:03 | <pre>Daytime: Cycle 39 started around 09:00 CLT, so most of CSCs were in Offline or Standby until it was done. \n",
       "\n",
       "We run successfully all the blocks. \n",
       "- BLOCK-T151 (1.0) Simonyi Hexapods Startup. Both M2/Camera Hexapods shows 17.3 C.\n",
       "- BLOCK-T3 (2.0) Warm-up M2 Hexapod.\n",
       "- BLOCK-T4 (2.0) Warm-up Camera Hexapod.\n",
       "- BLOCK-T153 - Opening M1M3 Mirror Covers.\n",
       "- BLOCK-T152  - Closing M1M3 Mirror Covers.\n",
       "- BLOCK-T145 - M1M3 Hardpoints Breakaway Test.\n",
       "- BLOCK-T144 - M1M3 Bump Test - Actuators [238, 330, 409] FAILED the bump test.\n",
       "- BLOCK-T139 - Hexapod Shutdown.\n",
       "\n",
       "The ScriptQueue:1 was updated and patched during the night. After the warm-up, M2 and Camera Hexapod were in disabled state during all the run and we monitor their temperature. Since the end of the warm-up until the hexapod shutdown their temperature increased ~ 1.5 deg C (average).</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:18:03 | <pre>Daytime: Cycle 39 started around 09:00 CLT, so most of CSCs were in Offline or Standby until it was done. Tony J. restart the ATcamera processes to prepare for cycle 39 upgrade. \n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:18:03 | <pre>Daytime: Cycle 39 started around 09:00 CLT, so most of CSCs were in Offline or Standby until it was done. Tony J. restart the auxtel camera processes to prepare for cycle 39 upgrade. \n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:18:03 | <pre>Daytime: Cycle 39 started around 09:00 CLT, so most of CSCs were in Offline or Standby until it was done. \n",
       "\n",
       "We run successfully all the blocks. \n",
       "- BLOCK-T151 (1.0) Simonyi Hexapods Startup. Both M2/Camera Hexapods shows 17.3 C\n",
       "- BLOCK-T3 (2.0) Warm-up M2 Hexapod\n",
       "- BLOCK-T4 (2.0) Warm-up Camera Hexapod\n",
       "- We set DISABLED state for both hexapod and monitor their temperature (temperature increased ~ 1.5 deg C from 18 to 19.4)\n",
       "- BLOCK-T153 - Opening M1M3 Mirror Covers\n",
       "- BLOCK-T152  - Closing M1M3 Mirror Covers \n",
       "- BLOCK-T145 - M1M3 Hardpoints Breakaway Test\n",
       "- Tiago updated ScriptQueue:1\n",
       "- BLOCK-T144 - M1M3 Bump Test - Actuators [238, 330, 409] FAILED the bump test\n",
       "- BLOCK-T139 - Hexapod Shutdown</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:18:03 | <pre>Daytime: Cycle 39 started around 09:00 CLT, so most of CSCs were in Offline or Standby until it was done. \n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:18:03 | <pre>Daytime: Cycle 39 started around 09:00 CLT, so most of CSCs were in Offline or Standby until it was done. \n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 13:44:14 | <pre>Daytime: Cycle 39 started around 09:00 CLT, so most of CSCs were in Offline or Standby until it was done. Tony J. restart the auxtel camera processes to prepare for cycle 39 upgrade. \n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 13:44:14 | <pre>Daytime: Cycle 39 started around 09:00 CLT, so most CSCs were Offline or Standby until it was done. Tony J. restarted the Auxtel camera processes to prepare for the cycle 39 upgrade. At the beginning of checkout, we had an image Timeout waiting for imageInOODS event issue with latiss_checkout. Steve P. helped figure it out and found that there was an issue with /data/staging/auxtel/oods at nfs-auxtel.cp.lsst.org, which was responding very slowly at that time. As this issue keeps happening (OBS-343), we shared this issue in #rubinobs-it-chile channel. \n",
       "\n",
       "While running power_on_atcalsys.py, we had an issue with one of the limit switches on the ATWhitelight (\"Shutter failed to open\" message on CSC) and it was solved when we manually moved the switch and made it touch the shutter. Latiss daytime calibration was done even though it failed at some attempts due to missing a patch for external scripts.  \n",
       "\n",
       "Around 20:40 pm CLT, AuxTel was ready to be on Sky and start the scheduler at 21 pm CLT after ATScriptQueue was updated. \n",
       "\n",
       "There is one created ticket to report an issue with linking the JIRA ticket to OLE (LOVE-362)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 13:44:14 | <pre>Daytime: Cycle 39 started around 09:00 CLT, so most of the CSCs were in Offline or Standby until it was done. Tony J. restarted the auxtel camera processes to prepare for the cycle 39 upgrade. At the beginning of checkout, we had an image Timeout waiting for imageInOODS event issue with latiss_checkout. Steve P. helped figure it out and found that there was an issue with /data/staging/auxtel/oods at nfs-auxtel.cp.lsst.org, which was responding very slowly at that time. As this issue keeps happening (OBS-343), we shared this issue in #rubinobs-it-chile channel. \n",
       "\n",
       "While running power_on_atcalsys.py, we had an issue with \n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 13:44:14 | <pre>Daytime: Cycle 39 started around 09:00 CLT, so most of the CSCs were in Offline or Standby until it was done. Tony J. restarted the Auxtel camera processes to prepare for the cycle 39 upgrade. At the beginning of checkout, we had an image Timeout waiting for imageInOODS event issue with latiss_checkout. Steve P. helped figure it out and found that there was an issue with /data/staging/auxtel/oods at nfs-auxtel.cp.lsst.org, which was responding very slowly at that time. As this issue keeps happening (OBS-343), we shared this issue in #rubinobs-it-chile channel. \n",
       "\n",
       "While running power_on_atcalsys.py, we had an issue with one of the limit switches on the ATWhitelight (\"Shutter failed to open\" message on CSC) and it was solved when we manually moved the switch and made it touch the shutter. Latiss daytime calibration was done even though it failed at the beginning due to missing a patch for external scripts.  \n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 13:44:14 | <pre>Daytime: Cycle 39 started around 09:00 CLT, so most CSCs were Offline or Standby until it was done. Tony J. restarted the Auxtel camera processes to prepare for the cycle 39 upgrade. At the beginning of checkout, we had an image Timeout waiting for imageInOODS event issue with latiss_checkout. Steve P. helped figure it out and found that there was an issue with /data/staging/auxtel/oods at nfs-auxtel.cp.lsst.org, which was responding very slowly at that time. As this issue keeps happening (OBS-343), we shared this issue in #rubinobs-it-chile channel. \n",
       "\n",
       "While running power_on_atcalsys.py, we had an issue with one of the limit switches on the ATWhitelight (\"Shutter failed to open\" message on CSC) and it was solved when we manually moved the switch and made it touch the shutter. Latiss daytime calibration was done even though it failed at some attempts due to missing a patch for external scripts.  \n",
       "\n",
       "Around 20:40 pm CLT, AuxTel was ready to be on sky and start the scheduler at pm CLT after ATScriptQueue updated. \n",
       "\n",
       "There is one created ticket to report an issue with linking JIRA ticket to OLE (LOVE-362)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 13:44:14 | <pre>Daytime: Cycle 39 started around 09:00 CLT, so most CSCs were Offline or Standby until it was done. Tony J. restarted the Auxtel camera processes to prepare for the cycle 39 upgrade. At the beginning of checkout, we had an image Timeout waiting for imageInOODS event issue with latiss_checkout. Steve P. helped figure it out and found that there was an issue with /data/staging/auxtel/oods at nfs-auxtel.cp.lsst.org, which was responding very slowly at that time. As this issue keeps happening (OBS-343), we shared this issue in #rubinobs-it-chile channel. \n",
       "\n",
       "While running power_on_atcalsys.py, we had an issue with one of the limit switches on the ATWhitelight (\"Shutter failed to open\" message on CSC) and it was solved when we manually moved the switch and made it touch the shutter. LATISS daytime calibrations were completed even though it failed at some attempts due to missing a patch for external scripts.  \n",
       "\n",
       "Around 20:40 pm CLT, AuxTel was ready to be on Sky and started the scheduler at 21 pm CLT after ATScriptQueue was updated and patched. \n",
       "\n",
       "There is one created ticket to report an issue on linking a JIRA ticket directly from OLE (LOVE-362).\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 13:44:14 | <pre>Daytime: Cycle 39 started around 09:00 CLT, so most CSCs were Offline or Standby until it was done. Tony J. restarted the Auxtel camera processes to prepare for the cycle 39 upgrade. At the beginning of checkout, we had an image Timeout waiting for imageInOODS event issue with latiss_checkout. Steve P. helped figure it out and found that there was an issue with /data/staging/auxtel/oods at nfs-auxtel.cp.lsst.org, which was responding very slowly at that time. As this issue keeps happening (OBS-343), we shared this issue in #rubinobs-it-chile channel. \n",
       "\n",
       "While running power_on_atcalsys.py, we had an issue with one of the limit switches on the ATWhitelight (\"Shutter failed to open\" message on CSC) and it was solved when we manually moved the switch and made it touch the shutter. LATISS daytime calibrations were completed even though it failed at some attempts due to missing a patch for external scripts.  \n",
       "\n",
       "Around 20:40 pm CLT, AuxTel was ready to be on Sky and started the scheduler at 21 pm CLT after ATScriptQueue was updated and patched. \n",
       "\n",
       "There is one created ticket to report an issue on linking a JIRA ticket directly from OLE (LOVE-362).\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 13:44:14 | <pre>Daytime: Cycle 39 started around 09:00 CLT, so most CSCs were Offline or Standby until it was done. Tony J. restarted the Auxtel camera processes to prepare for the cycle 39 upgrade. At the beginning of checkout, we had an image Timeout waiting for imageInOODS event issue with latiss_checkout. Steve P. helped figure it out and found that there was an issue with /data/staging/auxtel/oods at nfs-auxtel.cp.lsst.org, which was responding very slowly at that time. As this issue keeps happening (OBS-343), we shared this issue in #rubinobs-it-chile channel. \n",
       "\n",
       "While running power_on_atcalsys.py, we had an issue with one of the limit switches on the ATWhitelight (\"Shutter failed to open\" message on CSC) and it was solved when we manually moved the switch and made it touch the shutter. Latiss daytime calibration was done even though it failed at some attempts due to missing a patch for external scripts.  \n",
       "\n",
       "Around 20:40 pm CLT, AuxTel was ready to be on Sky and start the scheduler at 21 pm CLT after ATScriptQueue was updated. \n",
       "\n",
       "There is one created ticket to report an issue with linking the JIRA ticket to OLE (LOVE-362)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 13:44:14 | <pre>Daytime: Cycle 39 started around 09:00 CLT, so most CSCs were Offline or Standby until it was done. Tony J. restarted the Auxtel camera processes to prepare for the cycle 39 upgrade. At the beginning of checkout, we had an image Timeout waiting for imageInOODS event issue with latiss_checkout. Steve P. helped figure it out and found that there was an issue with /data/staging/auxtel/oods at nfs-auxtel.cp.lsst.org, which was responding very slowly at that time. As this issue keeps happening (OBS-343), we shared this issue in #rubinobs-it-chile channel. \n",
       "\n",
       "While running power_on_atcalsys.py, we had an issue with one of the limit switches on the ATWhitelight (\"Shutter failed to open\" message on CSC) and it was solved when we manually moved the switch and made it touch the shutter. Latiss daytime calibration was done even though it failed at some attempts due to missing a patch for external scripts.  \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "There is one created ticket to report an issue with linking JIRA ticket to OLE (LOVE-362)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 13:44:14 | <pre>Daytime: Cycle 39 started around 09:00 CLT, so most of the CSCs were in Offline or Standby until it was done. Tony J. restarted the Auxtel camera processes to prepare for the cycle 39 upgrade. At the beginning of checkout, we had an image Timeout waiting for imageInOODS event issue with latiss_checkout. Steve P. helped figure it out and found that there was an issue with /data/staging/auxtel/oods at nfs-auxtel.cp.lsst.org, which was responding very slowly at that time. As this issue keeps happening (OBS-343), we shared this issue in #rubinobs-it-chile channel. \n",
       "\n",
       "While running power_on_atcalsys.py, we had an issue with one of the limit switches on the ATWhitelight (\"Shutter failed to open\" message on CSC) and it was solved when we manually moved the switch and made it touch the shutter. Latiss daytime calibration was done even though it failed at the beginning due to missing a patch for external scripts.  \n",
       "\n",
       "\n",
       "\n",
       "There is one created ticket to report an issue with linking JIRA ticket to OLE (LOVE-362)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 13:44:14 | <pre>Daytime: Cycle 39 started around 09:00 CLT, so most of the CSCs were in Offline or Standby until it was done. Tony J. restarted the auxtel camera processes to prepare for the cycle 39 upgrade. At the beginning of checkout, we had an image Timeout waiting for imageInOODS event issue with latiss_checkout. Steve P. helped figure it out and found that there was an issue with /data/staging/auxtel/oods at nfs-auxtel.cp.lsst.org, which was responding very slowly at that time. As this issue keeps happening (OBS-343), we shared this issue in #rubinobs-it-chile channel. \n",
       "\n",
       "While running power_on_atcalsys.py, we had an issue with one of the limit switches on the ATWhitelight (\"Shutter failed to open\" message on CSC) and it was solved when we manually moved the switch and made it touch the shutter. Latiss daytime calibration was done even though it failed at the beginning due to missing a patch for external scripts.  \n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 13:44:14 | <pre>Daytime: Cycle 39 started around 09:00 CLT, so most of CSCs were in Offline or Standby until it was done. Tony J. restart the auxtel camera processes to prepare for cycle 39 upgrade. \n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## NIGHT: 20240917: "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### DATE: 2024-09-17: "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:45:42 | <pre>Daytime: In the morning Craig L. checked the limit switch (open) of the ATWhiteLight shutter and adjusted it. Now the limit switch works properly. Around 11:00 am CLT, Daytime checkout proceeded and was done without issue. Venting started around 11:30 am CLT. Around 15 pm CLT, we stopped venting and started calibration. Around 16:15pm CLT, we finished calibration and restarted venting until the beginning of the night. \n",
       "\n",
       "\n",
       "Nighttime: When preparing for on sky scheduler, Scheduler:2 went to FAULT state. This is currently a bug in the scheduler:2. The solution is to wait until there are targets available. \n",
       "\n",
       "We open the dome later (01:00 UT) because it was overcast.</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:45:42 | <pre>Daytime: In the morning Craig L. checked the limit switch (open) of the ATWhiteLight shutter and adjusted it. Now the limit switch works properly. Around 11:00 am CLT, Daytime checkout proceeded and was done without issue. Venting started around 11:30 am CLT. Around 15 pm CLT, we stopped venting and started calibration. Around 16:15pm CLT, we finished calibration and restarted venting until the beginning of the night. \n",
       "\n",
       "\n",
       "Nighttime: When preparing for on sky scheduler, Scheduler:2 went to FAULT state. There was a bug identified on the Scheduler:2 and now it is solved. On-sky observations started around 22:00 CLT when only thin clouds remained at the horizon.\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:45:42 | <pre>Daytime: In the morning Craig L. checked the limit switch (open) of the ATWhiteLight shutter and adjusted it. Now the limit switch works properly. Around 11:00 am CLT, Daytime checkout proceeded and was done without issue. Venting started around 11:30 am CLT. Around 15 pm CLT, we stopped venting and started calibration. Around 16:15pm CLT, we finished calibration and restarted venting until the beginning of the night. \n",
       "\n",
       "\n",
       "Nighttime: When preparing for on sky scheduler, Scheduler:2 went to FAULT state. This is currently a bug in the scheduler:2. The solution is to wait until there are targets available. We had to close the dome as it was overcast.</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:45:42 | <pre>Daytime: In the morning Craig L. checked the limit switch (open) of the ATWhiteLight shutter and adjusted it. Now the limit switch works properly. Around 11:00 am CLT, Daytime checkout proceeded and was done without issue. Venting started around 11:30 am CLT. Around 15 pm CLT, we stopped venting and started calibration. Around 16:15pm CLT, we finished calibration and restarted venting until the beginning of the night. \n",
       "\n",
       "\n",
       "Nighttime: Scheduler:2 went to fault </pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:45:42 | <pre>Daytime: In the morning Craig L. checked the limit switch (open) of the ATWhiteLight shutter and adjusted it. Now the limit switch works properly. Around 11:00am CLT, Daytime checkout proceeded and done without issue. Venting started around 11:30 am CLT. \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Nighttime:</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:45:42 | <pre>Daytime: In the morning Craig L. checked the limit switch (open) of the ATWhiteLight shutter and adjusted it. Now the limit switch works properly. Around 11:00 am CLT, Daytime checkout proceeded and was done without issue. Venting started around 11:30 am CLT. Around 15 pm CLT, we stopped venting and started calibration. \n",
       "\n",
       "\n",
       "\n",
       "Nighttime:</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:45:42 | <pre>Daytime:\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Nighttime:</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:45:42 | <pre>Daytime: In the morning Craig L. checked the limit switch (open) of the ATWhiteLight shutter and adjusted it. Now the limit switch works properly. Around 11:00 am CLT, Daytime checkout proceeded and was done without issue. Venting started around 11:30 am CLT. Around 15 pm CLT, we stopped venting and started calibration. Around 16:15pm CLT, we finished calibration and restarted venting until the beginning of the night. \n",
       "\n",
       "\n",
       "Nighttime: When preparing for on sky scheduler, Scheduler:2 went to FAULT state. There was a bug identified on the Scheduler:2 and now it is solved. On-sky observations started around 22:00 CLT when only thin clouds remained at the horizon.\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:45:42 | <pre>Daytime: In the morning Craig L. checked the limit switch (open) of the ATWhiteLight shutter and adjusted it. Now the limit switch works properly. Around 11:00 am CLT, Daytime checkout proceeded and was done without issue. Venting started around 11:30 am CLT. Around 15 pm CLT, we stopped venting and started calibration. Around 16:15pm CLT, we finished calibration and restarted venting until the beginning of the night. \n",
       "\n",
       "\n",
       "Nighttime:</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:45:42 | <pre></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:45:42 | <pre>Daytime: In the morning Craig L. checked the limit switch (open) of the ATWhiteLight shutter and adjusted it. Now the limit switch works properly. Around 11:00 am CLT, Daytime checkout proceeded and was done without issue. Venting started around 11:30 am CLT. Around 15 pm CLT, we stopped venting and started calibration. \n",
       "\n",
       "\n",
       "\n",
       "Nighttime:</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:46:16 | <pre>During daytime, Petr K. worked on M1M3 to run bump tests, raise the mirror, and test to find the reference/center position for static support centering (see more details and results at #m1m3_worklog channel). \n",
       "\n",
       "We started from Simonyi Hexapods Startup (BLOCK-T151) around 18:30 UTC and it was done without any issues. Then warm-up M2 Hexapod (BLOCK-T3) and warm Camera Hexapod (BLOCK-T4) were completed successfully. Both components were left at enabled state and were monitored. Strut12 from Camera Hexapod showed a notorious increase on temperature (about 2C in about 20 min plus diverting in motor current from the rest of the struts). It was turned off and left in Standby state.\n",
       "\n",
       "\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:46:16 | <pre>During daytime, Petr K. worked on M1M3 to run bump tests, raise the mirror, and test to find the reference/center position for static support centering (see more details and results at #m1m3_worklog channel). \n",
       "\n",
       "We started from Simonyi Hexapods Startup (BLOCK-T151) around 18:30 UTC and it was done without any issues. Then warm-up M2 Hexapod (BLOCK-T3) and warm Camera Hexapod (BLOCK-T4) were completed successfully. Both components were left at enabled state and were monitored.\n",
       "\n",
       "Camera Hexapod Strut12 showed a notorious increase on temperature (about 2C in about 20 min plus diverting in motor current from the rest of the struts). It was turned off and left in Standby state. \n",
       "\n",
       "M2 Hexapod, we kept monitoring its currents (as temperature sensors is not available).\n",
       "\n",
       "Actuators [218, 238, 409] FAILED the bump test. \n",
       "\n",
       "\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:46:16 | <pre>During daytime, Petr K. worked on M1M3 to run bump tests, raise the mirror, and test to find the reference/center position for static support centering (see more details and results at #m1m3_worklog channel). \n",
       "\n",
       "We started the Simonyi Hexapods Startup (BLOCK-T151) around 18:30 UTC and it was done without any issues. Then warm-up M2 Hexapod (BLOCK-T3) and warm Camera Hexapod (BLOCK-T4) were completed successfully. Both components were left at enabled state and were monitored (see OBS-592).\n",
       "\n",
       "Camera Hexapod Strut12 showed a notorious increase on temperature (about 2C in about 20 min plus diverting in motor current from the rest of the struts). It was turned off and left in standby state. \n",
       "\n",
       "For M2 Hexapod, we kept monitoring its currents on enabled state (as temperature sensors are not available, OBS-593). At about 23:30 CLT, motor current on leg 4 approached the 4 amp. We proceed to shutdown the hexapod but it was not possible (either locally nor remote) to access the PDU with the known credentials (IHS-8426).\n",
       "\n",
       "The remaining Simonyi tests (BLOCK-T145 M1M3 Hardpoints Breakaway Test and BLOCK-T144 M1M3 Bump Test) run successfully. Actuators [218, 238, 409] FAILED the bump test. \n",
       "\n",
       "\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:46:16 | <pre>During daytime, Petr K. worked on M1M3 to run bump tests, raise the mirror, and test to find the reference/center position for static support centering (see more details and results at #m1m3_worklog channel). \n",
       "\n",
       "We started from Simonyi Hexapods Startup (BLOCK-T151) around 18:30 UTC and it was done without any issues. Then warm-up M2 Hexapod (BLOCK-T3) and warm Camera Hexapod (BLOCK-T4) were completed successfully. Both components were left at enabled state and were monitored.\n",
       "\n",
       "Camera Hexapod Strut12 showed a notorious increase on temperature (about 2C in about 20 min plus diverting in motor current from the rest of the struts). It was turned off and left in Standby state. \n",
       "\n",
       "M2 Hexapod, we kept monitoring its currents (as temperature sensors are not available).\n",
       "\n",
       "BLOCK-T145 M1M3 Hardpoints Breakaway Test run successfully.\n",
       "\n",
       "BLOCK-T144 M1M3 Bump Test, Actuators [218, 238, 409] FAILED the bump test. \n",
       "\n",
       "\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:46:16 | <pre>During daytime, Petr K. worked on M1M3 to run bump tests, raise the mirror, and test to find the reference/center position for static support centering (see more details and results at #m1m3_worklog channel). \n",
       "\n",
       "We started the Simonyi Hexapods Startup (BLOCK-T151) around 18:30 UTC and it was done without any issues. Then warm-up M2 Hexapod (BLOCK-T3) and warm Camera Hexapod (BLOCK-T4) were completed successfully. Both components were left at enabled state and were monitored (see OBS-592).\n",
       "\n",
       "Camera Hexapod Strut12 showed a notorious increase on temperature (about 2C in about 20 min plus diverting in motor current from the rest of the struts). It was turned off and left in standby state. \n",
       "\n",
       "For M2 Hexapod, we kept monitoring its currents on enabled state (as temperature sensors are not available, OBS-593). At about 23:30 CLT, motor current on leg 4 approached the 4 amp. We proceed to shutdown the hexapod but it was not possible (either locally nor remote) to access the PDU with the known credentials (IHS-8426).\n",
       "\n",
       "The remaining Simonyi tests (BLOCK-T145 M1M3 Hardpoints Breakaway Test and BLOCK-T144 M1M3 Bump Test) run successfully. Actuators [218, 238, 409] FAILED the bump test. \n",
       "\n",
       "\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:46:16 | <pre>During daytime, Petr K. worked on M1M3 to run bump tests, raise the mirror, and test to find the reference/center position for static support centering (see more details and results at #m1m3_worklog channel). \n",
       "\n",
       "We started the Simonyi Hexapods Startup (BLOCK-T151) around 18:30 UTC and it was done without any issues. Then warm-up M2 Hexapod (BLOCK-T3) and warm Camera Hexapod (BLOCK-T4) were completed successfully. Both components were left at enabled state and were monitored.\n",
       "\n",
       "Camera Hexapod Strut12 showed a notorious increase on temperature (about 2C in about 20 min plus diverting in motor current from the rest of the struts). It was turned off and left in standby state. \n",
       "\n",
       "M2 Hexapod, we kept monitoring its currents (as temperature sensors are not available).\n",
       "\n",
       "The remaining Simonyi tests (BLOCK-T145 M1M3 Hardpoints Breakaway Test and BLOCK-T144 M1M3 Bump Test) run successfully. Actuators [218, 238, 409] FAILED the bump test. \n",
       "\n",
       "\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:46:16 | <pre>During daytime, Petr K. worked on M1M3 to run bump tests, raise the mirror, and test to find the reference/center position for static support centering (see more details and results at #m1m3_worklog channel). \n",
       "\n",
       "We started the Simonyi Hexapods Startup (BLOCK-T151) around 18:30 UTC and it was done without any issues. Then warm-up M2 Hexapod (BLOCK-T3) and warm Camera Hexapod (BLOCK-T4) were completed successfully. Both components were left at enabled state and were monitored (see OBS-592).\n",
       "\n",
       "Camera Hexapod Strut12 showed a notorious increase on temperature (about 2C in about 20 min plus diverting in motor current from the rest of the struts). It was turned off and left in standby state. \n",
       "\n",
       "For M2 Hexapod, we kept monitoring its currents on enabled state (as temperature sensors are not available, OBS-593). At about 23:30 CLT, motor current on leg 4 approached the 4 amp. We proceed to shutdown the hexapod but it was not possible (either locally nor remote) to access the PDU with the known credentials (IHS-8426).\n",
       "\n",
       "The remaining Simonyi tests (BLOCK-T145 M1M3 Hardpoints Breakaway Test and BLOCK-T144 M1M3 Bump Test) run successfully. Actuators [218, 238, 409] FAILED the bump test. \n",
       "\n",
       "\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:46:16 | <pre>We started from Simonyi Hexapods Startup (BLOCK-T151) around 18:30 UTC. </pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:46:16 | <pre>During daytime, Petr K. worked on M1M3 to run bump tests, raise the mirror, and test to find the reference/center position for static support centering (see more details and results at #m1m3_worklog channel). \n",
       "\n",
       "We started the Simonyi Hexapods Startup (BLOCK-T151) around 18:30 UTC and it was done without any issues. Then warm-up M2 Hexapod (BLOCK-T3) and warm Camera Hexapod (BLOCK-T4) were completed successfully. Both components were left at enabled state and were monitored (see OBS-592).\n",
       "\n",
       "Camera Hexapod Strut12 showed a notorious increase on temperature (about 2C in about 20 min plus diverting in motor current from the rest of the struts). It was turned off and left in standby state. \n",
       "\n",
       "For M2 Hexapod, we kept monitoring its currents (as temperature sensors are not available, OBS-593).\n",
       "\n",
       "The remaining Simonyi tests (BLOCK-T145 M1M3 Hardpoints Breakaway Test and BLOCK-T144 M1M3 Bump Test) run successfully. Actuators [218, 238, 409] FAILED the bump test. \n",
       "\n",
       "\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:46:16 | <pre>During daytime, Petr K. worked on M1M3 to run bump tests, raise the mirror, and test to find the reference/center position for static support centering (see more details and results at #m1m3_worklog channel). \n",
       "\n",
       "We started from Simonyi Hexapods Startup (BLOCK-T151) around 18:30 UTC and it was done without any issues. Then warm-up M2 Hexapod (BLOCK-T3) and warm Camera Hexapod (BLOCK-T4) were completed successfully. \n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:46:16 | <pre>During daytime, Petr K. worked on M1M3 to run bump tests, raise the mirror, and test to find the reference/center position for static support centering (see more details and results at #m1m3_worklog channel). \n",
       "\n",
       "We started from Simonyi Hexapods Startup (BLOCK-T151) around 18:30 UTC and it was done without any issues. Then warm-up M2 Hexapod (BLOCK-T3) and warm Camera Hexapod (BLOCK-T4) were completed successfully. Both components were left at enabled state and were monitored.\n",
       "\n",
       "Camera Hexapod Strut12 showed a notorious increase on temperature (about 2C in about 20 min plus diverting in motor current from the rest of the struts). It was turned off and left in Standby state. \n",
       "\n",
       "M2 Hexapod, we kept monitoring its currents (as temperature sensors are not available).\n",
       "\n",
       "BLOCK-T145 M1M3 Hardpoints Breakaway Test run successfully.\n",
       "\n",
       "BLOCK-T144 M1M3 Bump Test, Actuators [218, 238, 409] FAILED the bump test. \n",
       "\n",
       "\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:46:16 | <pre>During daytime, Petr K. worked on M1M3 to run bump tests, raise the mirror, and test to find the reference/center position for static support centering (see more details and results at #m1m3_worklog channel). \n",
       "\n",
       "We started from Simonyi Hexapods Startup (BLOCK-T151) around 18:30 UTC and it was done without any issues. Then warm-up M2 Hexapod (BLOCK-T3) and warm Camera Hexapod (BLOCK-T4) were completed successfully. Both components were left at enabled state and were monitored.\n",
       "\n",
       "Camera Hexapod Strut12 showed a notorious increase on temperature (about 2C in about 20 min plus diverting in motor current from the rest of the struts). It was turned off and left in Standby state. \n",
       "\n",
       "M2 Hexapod, we kept monitoring its currents (as temperature sensors is not available)\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:46:16 | <pre>During daytime, Petr K. worked on M1M3 to run bump tests, raise the mirror, and test to find the reference/center position for static support centering (see more details and results at #m1m3_worklog channel). \n",
       "\n",
       "We started the Simonyi Hexapods Startup (BLOCK-T151) around 18:30 UTC and it was done without any issues. Then warm-up M2 Hexapod (BLOCK-T3) and warm Camera Hexapod (BLOCK-T4) were completed successfully. Both components were left at enabled state and were monitored (see OBS-592).\n",
       "\n",
       "Camera Hexapod Strut12 showed a notorious increase on temperature (about 2C in about 20 min plus diverting in motor current from the rest of the struts). It was turned off and left in standby state. \n",
       "\n",
       "For M2 Hexapod, we kept monitoring its currents on enabled state (as temperature sensors are not available, OBS-593). At about 23:30 CLT, motor current on leg 4 approached the 4 amp. We proceed to shutdown the hexapod but it was not possible (either locally nor remote) to access the PDU with the known credentials (IHS-8426).\n",
       "\n",
       "The remaining Simonyi tests (BLOCK-T145 M1M3 Hardpoints Breakaway Test and BLOCK-T144 M1M3 Bump Test) run successfully. Actuators [218, 238, 409] FAILED the bump test. \n",
       "\n",
       "\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:46:16 | <pre></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 12:46:16 | <pre>During daytime, Petr K. worked on M1M3 to run bump tests, raise the mirror, and test to find the reference/center position for static support centering (see more details and results at #m1m3_worklog channel). \n",
       "\n",
       "We started from Simonyi Hexapods Startup (BLOCK-T151) around 18:30 UTC and it was done without any issues. Then warm-up M2 Hexapod (BLOCK-T3) and warm Camera Hexapod (BLOCK-T4) were completed successfully. \n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       ":EOT"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get data from Night Report log. Display nightly Jira BLOCKS.\n",
    "nr_adapter = sad.NightReportAdapter(server_url=server,\n",
    "                                    limit=limit,\n",
    "                                    min_day_obs=min_day_obs,\n",
    "                                    max_day_obs=max_day_obs,)\n",
    "nr_url = nr_adapter.source_url\n",
    "try:\n",
    "    nr_recs,nr_url = nr_adapter.get_reports()\n",
    "except Exception as err:\n",
    "    nr_recs = []\n",
    "    msg = f'ERROR getting records from {nr_url=}: {err=}'\n",
    "    raise Exception(msg)\n",
    "\n",
    "# print(f'Retrieved {len(nr_recs)} records from {nr_url}')\n",
    "\n",
    "# Display Jira BLOCKS\n",
    "front = 'https://rubinobs.atlassian.net/projects/BLOCK?selectedItem=com.atlassian.plugins.atlassian-connect-plugin:com.kanoah.test-manager__main-project-page#!/'\n",
    "tickets = nr_adapter.nightly_tickets(nr_recs)\n",
    "\n",
    "if tickets:\n",
    "    mdstr = '## Nightly Jira BLOCKs'\n",
    "    for day, url_list in tickets.items():\n",
    "        mdstr += f'\\n- {day}'\n",
    "        for ticket_url in url_list:\n",
    "            mdstr += f'\\n    - [{ticket_url.replace(front,\"\")}]({ticket_url})'\n",
    "    md(mdstr)\n",
    "else:\n",
    "    md(f'No jira BLOCK tickets found.', color='lightblue')\n",
    "    md(f'Used: [API Data]({nr_url})')\n",
    "\n",
    "# Display time log\n",
    "nr_rep = NightlyLogReport(min_day_obs=min_day_obs, max_day_obs=max_day_obs)\n",
    "nr_rep.time_log_as_markdown(nr_recs, nr_adapter, nr_url)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# Exposure Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### <font color='lightblue'>No Exposure Log records found.</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Used [API Data](https://summit-lsp.lsst.codes/exposurelog)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get data from Exposure log. Display time log.\n",
    "exposure_adapter = sad.ExposurelogAdapter(\n",
    "    server_url=server,\n",
    "    limit=limit,\n",
    "    min_day_obs=min_day_obs,\n",
    "    max_day_obs=max_day_obs,\n",
    ")\n",
    "exposure_url = exposure_adapter.source_url\n",
    "try:\n",
    "    exposure_recs,url = exposure_adapter.get_messages()\n",
    "except Exception as err:\n",
    "    exposure_recs = []\n",
    "    msg = f'ERROR getting records from {url=}: {err=}'\n",
    "    raise Exception(msg)\n",
    "\n",
    "if exposure_recs:\n",
    "    table = exposure_adapter.day_table(exposure_recs,'date_added', dayobs_field='day_obs')\n",
    "    #print(table)\n",
    "    mdlist(table)\n",
    "else:\n",
    "    md(f'No Exposure Log records found.', color='lightblue')\n",
    "    md(f'Used [API Data]({exposure_url})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exposure_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### <font color='lightblue'>No Observation Gaps found in exposures.</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display Observation gaps\n",
    "if usdf == os.environ.get('EXTERNAL_INSTANCE_URL'):\n",
    "    md(f\"**Warning:** The `/exposures/` endpoint is not yet functional on SERVER=usdf.\", color='red')\n",
    "gaps = exposure_adapter.get_observation_gaps()\n",
    "if gaps:\n",
    "    md(f'### Date vs Observation Gap (minutes) for all Instruments')\n",
    "    for instrument, day_gaps in gaps.items():\n",
    "        if len(day_gaps) == 0:\n",
    "            md(f'**No day gaps found for *{instrument=!s}* **', color='lightblue')\n",
    "        else:\n",
    "            x,y = zip(*day_gaps.items())\n",
    "            df = pd.DataFrame(dict(day=x,minutes=y))\n",
    "            df.plot.bar(x='day', y='minutes', title=f'{instrument=!s}')\n",
    "else:\n",
    "    md(f'No Observation Gaps found in exposures.', color='lightblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Narrative Log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### <font color='mediumblue'>Warning: Some text of Narrative log message may confuse markdown rendering.</font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## NIGHT: 20240916: "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### DATE: 2024-09-17: "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 02:05:26 | <pre><code>Resumed AuxTel on-sky\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 02:31:51 | <pre><code>Output of the Bump Test:\r\n",
       "2024/09/17 02:26:20 TAI\r\n",
       "\r\n",
       "\r\n",
       "RuntimeError: Actuators [238, 330, 409] FAILED the bump test.\r\n",
       "\r\n",
       "\r\n",
       "SAA (Single Actuator Axes) Failures:\r\n",
       "  None\r\n",
       "DAA (Dual Actuator Axes) Failures:\r\n",
       "  - Actuator ID 238: Pri Index 72\r\n",
       "  - Actuator ID 238: Sec Index 55\r\n",
       "  - Actuator ID 330: Pri Index 107\r\n",
       "  - Actuator ID 330: Pri Index 107\r\n",
       "  - Actuator ID 409: Pri Index 123\r\n",
       "  - Actuator ID 409: Pri Index 123\r\n",
       "\r\n",
       "\r\n",
       "\r\n",
       "\r\n",
       "\r\n",
       "\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 02:31:57 | <pre><code>Tiago is working on Scheduler:1 update.\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 02:57:00 | <pre><code>Tiago is working on Scheduler:1 update.& ScriptQueue:1\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 04:07:45 | <pre><code>ATPtg went to FAULT, message 'Azimuth out of range error''\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 04:13:21 | <pre><code>After recovering ATPtg the scheduler will not resume.\r\n",
       "\r\n",
       "\r\n",
       "SALindex: 200103 - TAI: 4:10 \r\n",
       "\r\n",
       "\r\n",
       "Error in run\r\n",
       "Traceback (most recent call last):\r\n",
       "  File \"/opt/lsst/software/stack/conda/envs/lsst-scipipe-9.0.0/lib/python3.11/site-packages/lsst/ts/salobj/base_script.py\", line 631, in do_run\r\n",
       "    await self._run_task\r\n",
       "  File \"/net/obs-env/auto_base_packages/ts_standardscripts/python/lsst/ts/standardscripts/scheduler/resume.py\", line 89, in run\r\n",
       "    await self.scheduler_remote.cmd_resume.set_start(timeout=self.timeout_start)\r\n",
       "  File \"/opt/lsst/software/stack/conda/envs/lsst-scipipe-9.0.0/lib/python3.11/site-packages/lsst/ts/salobj/topics/remote_command.py\", line 416, in set_start\r\n",
       "    return await self.start(timeout=timeout, wait_done=wait_done)\r\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/conda/envs/lsst-scipipe-9.0.0/lib/python3.11/site-packages/lsst/ts/salobj/topics/remote_command.py\", line 487, in start\r\n",
       "    return await cmd_info.next_ackcmd(timeout=timeout)\r\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/conda/envs/lsst-scipipe-9.0.0/lib/python3.11/site-packages/lsst/ts/salobj/topics/remote_command.py\", line 191, in next_ackcmd\r\n",
       "    raise base.AckError(msg=\"Command failed\", ackcmd=ackcmd)\r\n",
       "lsst.ts.salobj.base.AckError: msg='Command failed', ackcmd=(ackcmd private_seqNum=1364691750, ack=&lt;SalRetCode.CMD_FAILED: -302&gt;, error=1, result='Failed: Detailed state must be IDLE, currently in &lt;DetailedState.RUNNING: 2&gt;.')\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 04:15:42 | <pre><code>We changed  the scheduler:2 state to DISABLED and then ENABLED from the CSC summary.\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 04:21:25 | <pre><code>We changed  the scheduler:2 state to DISABLED and then ENABLED from the CSC summary and then we resumed the observations.\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 04:26:44 | <pre><code>Hexapod strut temperatures have increased about 1.5 C (average) since both warm-up finished. Both Hexapods are DISABLED.\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 04:44:09 | <pre><code>BLOCK-T139 (1.0) Simonyi Hexapod Shutdown completed\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 05:01:18 | <pre><code>Dome and telescope in the parking position, ATTCS and LATISS in Standby.\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## NIGHT: 20240917: "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### DATE: 2024-09-17: "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 15:25:39 | <pre><code>Vent started from 11:30 am CLT with vent.py. \r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 17:36:34 | <pre><code>Stop vent.py. Starting Daytime calibration. \r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 18:32:39 | <pre><code>Starting BLOCK-T151- Simonyi Hexapods Startup \r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 18:54:47 | <pre><code>Starting BLOCK-T3 (Warm-up M2 Hexapod \r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 18:55:05 | <pre><code>Starting BLOCK-T3 Warm-up M2 Hexapod \r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 19:04:19 | <pre><code>Starting BLOCK-T4 - Warm-up Camera Hexapod\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 19:07:04 | <pre><code>Starting BLOCK-T151- Simonyi Hexapods Startup.\r\n",
       "\r\n",
       "\r\n",
       "Camera hexapod temperature before started was 17.2 degrees  C\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 19:08:19 | <pre><code>Starting BLOCK-T151- Simonyi Hexapods Startup.\r\n",
       "\r\n",
       "\r\n",
       "Camera hexapod temperature was 17.2 degrees  C and begin to increase as soon  as it was startup.\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 19:22:21 | <pre><code>MT Hexapod went to FAUlt 19:14 TAI. \r\n",
       "'Force actuator Y 338 measured force (-372.00507 N) outside limits'\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 21:06:39 | <pre><code>Starting BLOCK-T153 Opening M1M3 Mirror Covers\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 21:11:02 | <pre><code>MTMount did not transition to Enabled state.  \r\n",
       "\r\n",
       "\r\n",
       "'begin_enable failed; remaining in state &lt;State.DISABLED: 1&gt;: The CSC was not allowed to command the mount: ExpectedError('Not connected to the low-level controller.')'\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 21:15:35 | <pre><code>TMA was in EUI mode, that's why we couldn't transition states\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 21:20:10 | <pre><code>MTM2 went to FAULT state \r\n",
       "\r\n",
       "\r\n",
       "'Lost the TCP/IP connection.'\r\n",
       "\r\n",
       "\r\n",
       "Error happens in the connection request.\r\n",
       "Traceback (most recent call last):\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/m2com/tcp_client.py\", line 127, in connect\r\n",
       "    await asyncio.wait_for(self.start_task, timeout=timeout)\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/asyncio/tasks.py\", line 489, in wait_for\r\n",
       "    return fut.result()\r\n",
       "           ^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/tcpip/client.py\", line 186, in start\r\n",
       "    reader, writer = await asyncio.open_connection(\r\n",
       "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/asyncio/streams.py\", line 48, in open_connection\r\n",
       "    transport, _ = await loop.create_connection(\r\n",
       "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/asyncio/base_events.py\", line 1086, in create_connection\r\n",
       "    raise exceptions[0]\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/asyncio/base_events.py\", line 1070, in create_connection\r\n",
       "    sock = await self._connect_sock(\r\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/asyncio/base_events.py\", line 974, in _connect_sock\r\n",
       "    await self.sock_connect(sock, address)\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/asyncio/selector_events.py\", line 638, in sock_connect\r\n",
       "    return await fut\r\n",
       "           ^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/asyncio/selector_events.py\", line 678, in _sock_connect_cb\r\n",
       "    raise OSError(err, f'Connect call failed {address}')\r\n",
       "ConnectionRefusedError: [Errno 111] Connect call failed ('139.229.178.194', 50011)\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 21:22:05 | <pre><code>MTM2 is now ENABLED\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 21:30:05 | <pre><code>BLOCK-T152  Closing M1M3 Mirror Covers run succesfully\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 21:30:26 | <pre><code>Running BLOCK-T145 M1M3 Hardpoints Breakaway\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 21:34:19 | <pre><code>Camera hexapod strut 12 temperature is increasing rapidly unlike other temperatures \r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 21:37:22 | <pre><code>Camera hexapod strut 12 temperature is rising rapidly, unlike the other strut temperature, which are increasing more gradually.\r\n",
       "\r\n",
       "\r\n",
       "after warmup it was 17 degrees C  and it reached  19.3 degrees C  \r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 21:39:08 | <pre><code>Camera hexapod shutdown and set to STANDBY state\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 22:40:57 | <pre><code>Scheduler:2 went to FAULT state \r\n",
       "\r\n",
       "\r\n",
       "Error on advance target production loop.\r\n",
       "Traceback (most recent call last):\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 1463, in advance_target_production_loop\r\n",
       "    await self.generate_target_queue()\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/utils/csc_utils.py\", line 200, in detailed_state_wrapper\r\n",
       "    await coroutine(self, *args, **kwargs)\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 1641, in generate_target_queue\r\n",
       "    await self.handle_no_targets_on_queue()\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 1733, in handle_no_targets_on_queue\r\n",
       "    await self.put_on_queue([stop_tracking_target])\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 872, in put_on_queue\r\n",
       "    async for sal_index in self._queue_block_scripts(observing_block):\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 928, in _queue_block_scripts\r\n",
       "    sal_index = await self._queue_one_script(\r\n",
       "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 984, in _queue_one_script\r\n",
       "    add_task = await self.queue_remote.cmd_add.set_start(\r\n",
       "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/salobj/topics/remote_command.py\", line 416, in set_start\r\n",
       "    return await self.start(timeout=timeout, wait_done=wait_done)\r\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/salobj/topics/remote_command.py\", line 487, in start\r\n",
       "    return await cmd_info.next_ackcmd(timeout=timeout)\r\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/salobj/topics/remote_command.py\", line 191, in next_ackcmd\r\n",
       "    raise base.AckError(msg=\"Command failed\", ackcmd=ackcmd)\r\n",
       "lsst.ts.salobj.base.AckError: msg='Command failed', ackcmd=(ackcmd private_seqNum=78556861, ack=&lt;SalRetCode.CMD_FAILED: -302&gt;, error=1, result='Failed: _Internal has the wrong format, should be BLOCK-N or BLOCK-TN.')\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 22:50:18 | <pre><code>Scheduler:2 went to FAULT state after resume.py 3 times, after enable it from ATQueue.\r\n",
       "\r\n",
       "\r\n",
       "Error on advance target production loop.\r\n",
       "Traceback (most recent call last):\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 1463, in advance_target_production_loop\r\n",
       "    await self.generate_target_queue()\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/utils/csc_utils.py\", line 200, in detailed_state_wrapper\r\n",
       "    await coroutine(self, *args, **kwargs)\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 1641, in generate_target_queue\r\n",
       "    await self.handle_no_targets_on_queue()\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 1733, in handle_no_targets_on_queue\r\n",
       "    await self.put_on_queue([stop_tracking_target])\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 872, in put_on_queue\r\n",
       "    async for sal_index in self._queue_block_scripts(observing_block):\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 928, in _queue_block_scripts\r\n",
       "    sal_index = await self._queue_one_script(\r\n",
       "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 984, in _queue_one_script\r\n",
       "    add_task = await self.queue_remote.cmd_add.set_start(\r\n",
       "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/salobj/topics/remote_command.py\", line 416, in set_start\r\n",
       "    return await self.start(timeout=timeout, wait_done=wait_done)\r\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/salobj/topics/remote_command.py\", line 487, in start\r\n",
       "    return await cmd_info.next_ackcmd(timeout=timeout)\r\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/salobj/topics/remote_command.py\", line 191, in next_ackcmd\r\n",
       "    raise base.AckError(msg=\"Command failed\", ackcmd=ackcmd)\r\n",
       "lsst.ts.salobj.base.AckError: msg='Command failed', ackcmd=(ackcmd private_seqNum=78556861, ack=&lt;SalRetCode.CMD_FAILED: -302&gt;, error=1, result='Failed: _Internal has the wrong format, should be BLOCK-N or BLOCK-TN.')\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 22:52:36 | <pre><code>Scheduler:2 went to FAULT state after resume.py 3 times, after enable it from ATQueue. This is currently a bug in the update  that needs to be fix.\r\n",
       "\r\n",
       "\r\n",
       "Error on advance target production loop.\r\n",
       "Traceback (most recent call last):\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 1463, in advance_target_production_loop\r\n",
       "    await self.generate_target_queue()\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/utils/csc_utils.py\", line 200, in detailed_state_wrapper\r\n",
       "    await coroutine(self, *args, **kwargs)\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 1641, in generate_target_queue\r\n",
       "    await self.handle_no_targets_on_queue()\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 1733, in handle_no_targets_on_queue\r\n",
       "    await self.put_on_queue([stop_tracking_target])\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 872, in put_on_queue\r\n",
       "    async for sal_index in self._queue_block_scripts(observing_block):\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 928, in _queue_block_scripts\r\n",
       "    sal_index = await self._queue_one_script(\r\n",
       "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 984, in _queue_one_script\r\n",
       "    add_task = await self.queue_remote.cmd_add.set_start(\r\n",
       "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/salobj/topics/remote_command.py\", line 416, in set_start\r\n",
       "    return await self.start(timeout=timeout, wait_done=wait_done)\r\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/salobj/topics/remote_command.py\", line 487, in start\r\n",
       "    return await cmd_info.next_ackcmd(timeout=timeout)\r\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/salobj/topics/remote_command.py\", line 191, in next_ackcmd\r\n",
       "    raise base.AckError(msg=\"Command failed\", ackcmd=ackcmd)\r\n",
       "lsst.ts.salobj.base.AckError: msg='Command failed', ackcmd=(ackcmd private_seqNum=78556861, ack=&lt;SalRetCode.CMD_FAILED: -302&gt;, error=1, result='Failed: _Internal has the wrong format, should be BLOCK-N or BLOCK-TN.')\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 22:54:57 | <pre><code>Scheduler:2 went to FAULT state in the  resume.py 3 times, after enable it from ATQueue. This is currently a bug in the shceduler:2 update that needs to be fix. The solution is to wait until there are targets available.\r\n",
       "\r\n",
       "\r\n",
       "First occurrence  TAI:  22:39 (the script did not fault)\r\n",
       "\r\n",
       "\r\n",
       "Error on advance target production loop.\r\n",
       "Traceback (most recent call last):\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 1463, in advance_target_production_loop\r\n",
       "    await self.generate_target_queue()\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/utils/csc_utils.py\", line 200, in detailed_state_wrapper\r\n",
       "    await coroutine(self, *args, **kwargs)\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 1641, in generate_target_queue\r\n",
       "    await self.handle_no_targets_on_queue()\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 1733, in handle_no_targets_on_queue\r\n",
       "    await self.put_on_queue([stop_tracking_target])\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 872, in put_on_queue\r\n",
       "    async for sal_index in self._queue_block_scripts(observing_block):\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 928, in _queue_block_scripts\r\n",
       "    sal_index = await self._queue_one_script(\r\n",
       "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 984, in _queue_one_script\r\n",
       "    add_task = await self.queue_remote.cmd_add.set_start(\r\n",
       "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/salobj/topics/remote_command.py\", line 416, in set_start\r\n",
       "    return await self.start(timeout=timeout, wait_done=wait_done)\r\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/salobj/topics/remote_command.py\", line 487, in start\r\n",
       "    return await cmd_info.next_ackcmd(timeout=timeout)\r\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/salobj/topics/remote_command.py\", line 191, in next_ackcmd\r\n",
       "    raise base.AckError(msg=\"Command failed\", ackcmd=ackcmd)\r\n",
       "lsst.ts.salobj.base.AckError: msg='Command failed', ackcmd=(ackcmd private_seqNum=78556861, ack=&lt;SalRetCode.CMD_FAILED: -302&gt;, error=1, result='Failed: _Internal has the wrong format, should be BLOCK-N or BLOCK-TN.')\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 23:22:41 | <pre><code>We are running BLOCK-T144 M1M3 Bump Test\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 00:02:15 | <pre><code>AuxTel is closed due to bad weather conditions\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 00:28:44 | <pre><code>Actuators [218, 238, 409] FAILED the bump test.\r\n",
       "\r\n",
       "\r\n",
       "SAA (Single Actuator Axes) Failures:\r\n",
       "  None\r\n",
       "DAA (Dual Actuator Axes) Failures:\r\n",
       "  - Actuator ID 218: Pri Index 53\r\n",
       "  - Actuator ID 218: Pri Index 53\r\n",
       "  - Actuator ID 238: Pri Index 72\r\n",
       "  - Actuator ID 238: Sec Index 55\r\n",
       "  - Actuator ID 409: Pri Index 123\r\n",
       "  - Actuator ID 409: Pri Index 123\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 00:32:46 | <pre><code>Scheduler:2 went to FAULT state during the  resume.py 3 times, after enable it from ATQueue. This is currently a bug in the scheduler:2 update.The solution is to wait until there are targets available.\r\n",
       "\r\n",
       "\r\n",
       "First occurrence  TAI:  22:39 (the script did not fault)\r\n",
       "\r\n",
       "\r\n",
       "Error on advance target production loop.\r\n",
       "Traceback (most recent call last):\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 1463, in advance_target_production_loop\r\n",
       "    await self.generate_target_queue()\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/utils/csc_utils.py\", line 200, in detailed_state_wrapper\r\n",
       "    await coroutine(self, *args, **kwargs)\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 1641, in generate_target_queue\r\n",
       "    await self.handle_no_targets_on_queue()\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 1733, in handle_no_targets_on_queue\r\n",
       "    await self.put_on_queue([stop_tracking_target])\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 872, in put_on_queue\r\n",
       "    async for sal_index in self._queue_block_scripts(observing_block):\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 928, in _queue_block_scripts\r\n",
       "    sal_index = await self._queue_one_script(\r\n",
       "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/scheduler/scheduler_csc.py\", line 984, in _queue_one_script\r\n",
       "    add_task = await self.queue_remote.cmd_add.set_start(\r\n",
       "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/salobj/topics/remote_command.py\", line 416, in set_start\r\n",
       "    return await self.start(timeout=timeout, wait_done=wait_done)\r\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/salobj/topics/remote_command.py\", line 487, in start\r\n",
       "    return await cmd_info.next_ackcmd(timeout=timeout)\r\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
       "  File \"/opt/lsst/software/stack/miniconda/lib/python3.11/site-packages/lsst/ts/salobj/topics/remote_command.py\", line 191, in next_ackcmd\r\n",
       "    raise base.AckError(msg=\"Command failed\", ackcmd=ackcmd)\r\n",
       "lsst.ts.salobj.base.AckError: msg='Command failed', ackcmd=(ackcmd private_seqNum=78556861, ack=&lt;SalRetCode.CMD_FAILED: -302&gt;, error=1, result='Failed: _Internal has the wrong format, should be BLOCK-N or BLOCK-TN.')\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 00:37:13 | <pre><code>M1M3 went to FAULT state 'Force actuator Y 338 measured force (-372.40256 N) outside limits'\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 00:52:10 | <pre><code>Preparing to go on sky\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 02:58:53 | <pre><code>M2 hexapod set to standby state\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 03:18:14 | <pre><code>We could not login to  https://pdu1-tea-as02.cp.lsst.org to turn off the drives and PXI. \r\n",
       "Even though we were able to do so earlier.\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 03:31:55 | <pre><code>M2 Hexapod set to disabled state\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 03:33:57 | <pre><code>M2 Hexapod set to disabled state. \r\n",
       "Currents now  is around 0 \r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 03:38:43 | <pre><code>M2 Hexapod set to disabled state. \r\n",
       "Currents now are around 0 \r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 04:41:06 | <pre><code>Stopping observations and will begin shutdown\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 04:53:33 | <pre><code>AuxTel Dome and Telescope at parking position.\r\n",
       "ATTCS and LATISS components in Standby.\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 04:54:04 | <pre><code>Final status: \r\n",
       "MTMount, M1M3 disabled.\r\n",
       "MTPtg &amp; MTM2 enabled.\r\n",
       "Camera Hexapod in Standby state with drives and PXI turned off.\r\n",
       "M2 Hexapod in Disabled state with drives and PXI turned on.\r\n",
       "</code></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       ":EOT"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get data from Narrative log. Display time log.\n",
    "narrative_adapter = sad.NarrativelogAdapter(\n",
    "    server_url=server,\n",
    "    limit=limit,\n",
    "    min_day_obs=min_day_obs,\n",
    "    max_day_obs=max_day_obs,\n",
    ")\n",
    "narrative_url = narrative_adapter.source_url\n",
    "try:\n",
    "    # date like '2000-01-02 12:00:00'\n",
    "    # str(datetime(2000, 1, 2, 12, 0, 0))\n",
    "    min_date = str(datetime.strptime(min_day_obs,'%Y%m%d'))\n",
    "    max_date = str(datetime.strptime(max_day_obs,'%Y%m%d'))\n",
    "    #!print(f'Get data from {narrative_url}: {min_date} to {max_date}')\n",
    "    narrative_recs,url = narrative_adapter.get_messages()\n",
    "except Exception as err:\n",
    "    narrative_recs = []\n",
    "    msg = f'ERROR getting records from {url}: {err=}'\n",
    "    raise Exception(msg)\n",
    "\n",
    "# print(f'Retrieved {len(narrative_recs)} records.')\n",
    "\n",
    "if narrative_recs:\n",
    "    md('Warning: Some text of Narrative log message may confuse markdown rendering.',\n",
    "      color='mediumblue')\n",
    "    table = narrative_adapter.day_table(narrative_recs, 'date_added')\n",
    "    #print(tabstr)\n",
    "    #mdlist(table, color=\"darkblue\")\n",
    "    mdlist(table)\n",
    "else:\n",
    "    md(f'No Narrative Log records found.', color='lightblue')\n",
    "    md(f'Used [API Data]({narrative_url})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Dashboard"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "(This is not done when running under Times Square.)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try connect to each endpoint of https://summit-lsp.lsst.codes/exposurelog \n",
      "Try connect to each endpoint of https://summit-lsp.lsst.codes/narrativelog \n",
      "Try connect to each endpoint of https://summit-lsp.lsst.codes/nightreport \n",
      "Try connect to each endpoint of https://usdf-rsp-dev.slac.stanford.edu/exposurelog \n",
      "Try connect to each endpoint of https://usdf-rsp-dev.slac.stanford.edu/narrativelog \n",
      "Try connect to each endpoint of https://usdf-rsp-dev.slac.stanford.edu/nightreport \n",
      "Try connect to each endpoint of https://tucson-teststand.lsst.codes/exposurelog \n",
      "Try connect to each endpoint of https://tucson-teststand.lsst.codes/narrativelog \n",
      "Try connect to each endpoint of https://tucson-teststand.lsst.codes/nightreport \n",
      "\n",
      "Connected to 9 out of 15 endpoints.(60%)\n",
      "Successful connects (9): \n",
      "\thttps://summit-lsp.lsst.codes/exposurelog/instruments\n",
      "\thttps://summit-lsp.lsst.codes/exposurelog/exposures?instrument=na\n",
      "\thttps://summit-lsp.lsst.codes/exposurelog/messages\n",
      "\thttps://summit-lsp.lsst.codes/narrativelog/messages\n",
      "\thttps://summit-lsp.lsst.codes/nightreport/reports\n",
      "\thttps://usdf-rsp-dev.slac.stanford.edu/exposurelog/messages\n",
      "\thttps://usdf-rsp-dev.slac.stanford.edu/narrativelog/messages\n",
      "\thttps://tucson-teststand.lsst.codes/exposurelog/instruments\n",
      "\thttps://tucson-teststand.lsst.codes/exposurelog/exposures?instrument=na\n",
      "Failed connects (6): \n",
      "\t500: https://usdf-rsp-dev.slac.stanford.edu/exposurelog/instruments\n",
      "\t500: https://usdf-rsp-dev.slac.stanford.edu/exposurelog/exposures?instrument=na\n",
      "\t404: https://usdf-rsp-dev.slac.stanford.edu/nightreport/reports\n",
      "\t500: https://tucson-teststand.lsst.codes/exposurelog/messages\n",
      "\t500: https://tucson-teststand.lsst.codes/narrativelog/messages\n",
      "\t500: https://tucson-teststand.lsst.codes/nightreport/reports\n",
      "score=60%\n",
      "Servers that are fully functional for Logging and Reporting:\n",
      "\t https://summit-lsp.lsst.codes\n"
     ]
    }
   ],
   "source": [
    "# Conditionally display our current ability to connect to all needed endpoints.\n",
    "if not os.environ.get('EXTERNAL_INSTANCE_URL'):\n",
    "    md('# Dashboard')\n",
    "    md('(This is not done when running under Times Square.)')\n",
    "    %run ./dashboard.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 2024-09-18 14:53:49.364756\n"
     ]
    }
   ],
   "source": [
    "print(f'Finished {str(datetime.now())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vbecker/Develop/lsst/ts_repos/ts_logging_and_reporting/notebooks_tsqr\n",
      "['NightLog.ipynb', 'logrep_proto_1.ipynb', 'requirements.txt', 'NightLog.yaml', 'efd.ipynb', 'NightLog2.ipynb', 'scaffolding.org', 'README.md', 'dashboard.ipynb', 'exposurelog.HIDE_yaml', 'consdb', '.ipynb_checkpoints', 'TEMPLATE_logrep.HIDE_yaml', 'logrep_all_env.ipynb', 'narrativelog.HIDE_yaml', 'efd.HIDE_yaml', 'narrativelog.ipynb', 'exposurelog.ipynb', 'TEMPLATE_logrep.ipynb']\n",
      "None\n",
      "Installing \"lsst.sitcom.summit.utils\" from github using \"main\" branch....\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/lsst-sitcom/summit_utils.git /private/var/folders/s5/114qkzh55t36hvzbb6blzdq80000gp/T/pip-req-build-tpomyp4v\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[14 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m error: Multiple top-level packages discovered in a flat-layout: ['ups', 'config', 'pipelines'].\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m To avoid accidental inclusion of unwanted files or directories,\n",
      "  \u001b[31m   \u001b[0m setuptools will not proceed with this build.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you are trying to create a single distribution with multiple packages\n",
      "  \u001b[31m   \u001b[0m on purpose, you should not rely on automatic discovery.\n",
      "  \u001b[31m   \u001b[0m Instead, consider the following options:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m 1. set up custom discovery (`find` directive with `include` or `exclude`)\n",
      "  \u001b[31m   \u001b[0m 2. use a `src-layout`\n",
      "  \u001b[31m   \u001b[0m 3. explicitly set `py_modules` or `packages` with a list of names\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m To find more information, look for \"package discovery\" on setuptools docs.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lsst.summit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/var/folders/s5/114qkzh55t36hvzbb6blzdq80000gp/T/ipykernel_58071/917285549.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstalling \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlsst.sitcom.summit.utils\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m from github using \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m branch....\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall --upgrade git+https://github.com/lsst-sitcom/summit_utils.git@main>/dev/null\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlsst\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConsDbClient\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lsst.summit'"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lsst.summit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mgetcwd())\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir())\n\u001b[0;32m----> 3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconsdb/assorted_plots.ipynb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/logrep/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2480\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2478\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2480\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2484\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/logrep/lib/python3.12/site-packages/IPython/core/magics/execution.py:737\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m preserve_keys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m filename\n\u001b[0;32m--> 737\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39msafe_execfile_ipy(filename, raise_exceptions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;66;03m# Control the response to exit() calls made by the script being run\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/logrep/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3005\u001b[0m, in \u001b[0;36mInteractiveShell.safe_execfile_ipy\u001b[0;34m(self, fname, shell_futures, raise_exceptions)\u001b[0m\n\u001b[1;32m   3003\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_cell(cell, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shell_futures\u001b[38;5;241m=\u001b[39mshell_futures)\n\u001b[1;32m   3004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_exceptions:\n\u001b[0;32m-> 3005\u001b[0m     result\u001b[38;5;241m.\u001b[39mraise_error()\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[1;32m   3007\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/logrep/lib/python3.12/site-packages/IPython/core/interactiveshell.py:308\u001b[0m, in \u001b[0;36mExecutionResult.raise_error\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_before_exec\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/var/folders/s5/114qkzh55t36hvzbb6blzdq80000gp/T/ipykernel_58071/917285549.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstalling \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlsst.sitcom.summit.utils\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m from github using \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m branch....\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall --upgrade git+https://github.com/lsst-sitcom/summit_utils.git@main>/dev/null\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlsst\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConsDbClient\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lsst.summit'"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "print(os.listdir())\n",
    "%run consdb/assorted_plots.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
