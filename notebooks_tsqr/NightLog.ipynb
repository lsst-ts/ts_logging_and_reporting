{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Night Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters. Set defaults here.\n",
    "# Times Square replaces this cell with the user's parameters.\n",
    "record_limit = '999'\n",
    "day_obs = 'TODAY'  # TODAY, YESTERDAY, YYYY-MM-DD\n",
    "number_of_days = '1'  # Total number of days of data to display (ending on day_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use packages available in the Rubin Science Platform\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from pprint import pp\n",
    "from urllib.parse import urlencode\n",
    "from IPython.display import display, Markdown, display_markdown\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from datetime import datetime, date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = int(record_limit)\n",
    "\n",
    "match day_obs.lower():\n",
    "    case 'today':\n",
    "        date = datetime.now().date()\n",
    "    case 'yesterday':\n",
    "        date = datetime.now().date()-timedelta(days=1)\n",
    "    case _:\n",
    "        date = datetime.strptime(dd, '%Y-%m-%d').date()\n",
    "\n",
    "days = int(number_of_days)\n",
    "\n",
    "# Thus: [min_day_obs,max_day_obs)\n",
    "min_day_obs = (date - timedelta(days=days-1)).strftime('%Y%m%d') # Inclusive\n",
    "max_day_obs = (date + timedelta(days=1)).strftime('%Y%m%d') # prep for Exclusive\n",
    "\n",
    "response_timeout = 3.05  # seconds, how long to wait for connection\n",
    "read_timeout = 20  # seconds\n",
    "timeout = (float(response_timeout), float(read_timeout))\n",
    "\n",
    "summit = 'https://summit-lsp.lsst.codes'\n",
    "usdf = 'https://usdf-rsp-dev.slac.stanford.edu'\n",
    "tucson = 'https://tucson-teststand.lsst.codes'\n",
    "\n",
    "# Use server=tucson for dev testing.\n",
    "# Use server=usdf for push to develop.   TODO\n",
    "server = os.environ.get('EXTERNAL_INSTANCE_URL', tucson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Report from {server} over {number_of_days} nights'\n",
    "      f' from {min_day_obs} to {date}. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Times Square, comment out next line and past next cell with contents of local python file.\n",
    "#! from lsst.ts.logging_and_reporting.source_adapters import ExposurelogAdapter, NarrativelogAdapter, keep_fields\n",
    "# Once our logrep package has been installed in RSP, we can use the simpler \"import\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Comment out the import in the cell above this one.\n",
    "# Paste contents of source_adapters.py in new cell below this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is part of ts_logging_and_reporting.\n",
    "#\n",
    "# Developed for Vera C. Rubin Observatory Telescope and Site Systems.\n",
    "# This product includes software developed by the LSST Project\n",
    "# (https://www.lsst.org).\n",
    "# See the COPYRIGHT file at the top-level directory of this distribution\n",
    "# for details of code ownership.\n",
    "#\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
    "\n",
    "\n",
    "############################################\n",
    "# Python Standard Library\n",
    "from urllib.parse import urlencode\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from warnings import warn\n",
    "from collections import defaultdict\n",
    "from abc import ABC\n",
    "\n",
    "############################################\n",
    "# External Packages\n",
    "import requests\n",
    "\n",
    "MAX_CONNECT_TIMEOUT = 3.1    # seconds\n",
    "MAX_READ_TIMEOUT = 90 * 60   # seconds\n",
    "\n",
    "def keep_fields(outfields, recs):\n",
    "    \"\"\"Keep only keys in OUTFIELDS list of RECS (list of dicts)\n",
    "    SIDE EFFECT: Removes extraneous keys from all dicts in RECS.\n",
    "    \"\"\"\n",
    "    if outfields:\n",
    "        for rec in recs:\n",
    "            nukefields = set(rec.keys()) - set(outfields)\n",
    "            print(f'{rec=} {nukefields=}')\n",
    "            for f in nukefields:\n",
    "                del rec[f]\n",
    "\n",
    "class SourceAdapter(ABC):\n",
    "    \"\"\"Abstract Base Class for all source adapters.\n",
    "    \"\"\"\n",
    "    # TODO document class including all class variables.\n",
    "    def __init__(self, *,\n",
    "                 server_url='https://tucson-teststand.lsst.codes',\n",
    "                 connect_timeout=1.05,  # seconds\n",
    "                 read_timeout=2,  # seconds\n",
    "                 ):\n",
    "        self.server = server_url\n",
    "        self.c_timeout = min(MAX_CONNECT_TIMEOUT,\n",
    "                             float(connect_timeout))  # seconds\n",
    "        self.r_timeout = min(MAX_READ_TIMEOUT,  # seconds\n",
    "                             float(read_timeout))\n",
    "        self.timeout = (self.c_timeout, self.r_timeout)\n",
    "\n",
    "        # Provide the following in subclass\n",
    "        output_fields = None\n",
    "        service = None\n",
    "        endpoints = None\n",
    "\n",
    "    @property\n",
    "    def source_url(self):\n",
    "        return f'{self.server}/{self.service}'\n",
    "\n",
    "\n",
    "    def check_endpoints(self, timeout=None):\n",
    "        to = (timeout or self.timeout)\n",
    "        print(f'Try connect to each endpoint of {self.server}/{self.service} '\n",
    "              f'using timeout={to}.')\n",
    "        url_http_status_code = dict()\n",
    "        for ep in self.endpoints:\n",
    "            url = f'{self.server}/{self.service}/{ep}'\n",
    "            try:\n",
    "                r = requests.get(url, timeout=(timeout or self.timeout))\n",
    "            except:\n",
    "                url_http_status_code[url] = 'timeout'\n",
    "            else:\n",
    "                url_http_status_code[url] = r.status_code\n",
    "        return url_http_status_code\n",
    "\n",
    "\n",
    "    def analytics(self, recs, categorical_fields=None):\n",
    "        if len(recs) == 0:\n",
    "            return dict(fields=[],\n",
    "                        facet_fields=set(),\n",
    "                        facets=dict())\n",
    "\n",
    "        non_cats = set([\n",
    "            'tags', 'urls', 'message_text', 'id', 'date_added',\n",
    "            'obs_id', 'day_obs', 'seq_num', 'parent_id', 'user_id',\n",
    "            'date_invalidated', 'date_begin', 'date_end',\n",
    "            'time_lost', # float\n",
    "            # 'systems','subsystems','cscs',  # values need special handling\n",
    "        ])\n",
    "        flds = set(recs[0].keys())\n",
    "        if not categorical_fields:\n",
    "            categorical_fields = flds\n",
    "        ignore_fields = flds - categorical_fields\n",
    "        facflds = flds - ignore_fields\n",
    "\n",
    "        # facets(field) = set(value-1, value-2, ...)\n",
    "        facets = {fld: set([str(r[fld])\n",
    "                    for r in recs if not isinstance(r[fld], list)])\n",
    "                    for fld in facflds}\n",
    "        return dict(fields=flds,\n",
    "                    facet_fields=facflds,\n",
    "                    facets=facets,\n",
    "                    )\n",
    "\n",
    "\n",
    "# Not available on SLAC (usdf) as of 9/9/2024.\n",
    "class NightReportAdapter(SourceAdapter):\n",
    "    service = \"nightreport\"\n",
    "    endpoints = ['reports']\n",
    "    primary_endpoint = 'reports'\n",
    "\n",
    "class NarrativelogAdapter(SourceAdapter):\n",
    "    \"\"\"TODO full documentation\n",
    "    \"\"\"\n",
    "    service = 'narrativelog'\n",
    "    endpoints = ['messages',]\n",
    "    primary_endpoint = 'messages'\n",
    "    fields = {'category',\n",
    "              'components',\n",
    "              'cscs',\n",
    "              'date_added',\n",
    "              'date_begin',\n",
    "              'date_end',\n",
    "              'date_invalidated',\n",
    "              'id',\n",
    "              'is_human',\n",
    "              'is_valid',\n",
    "              'level',\n",
    "              'message_text',\n",
    "              'parent_id',\n",
    "              'primary_hardware_components',\n",
    "              'primary_software_components',\n",
    "              'site_id',\n",
    "              'subsystems',\n",
    "              'systems',\n",
    "              'tags',\n",
    "              'time_lost',\n",
    "              'time_lost_type',\n",
    "              'urls',\n",
    "              'user_agent',\n",
    "              'user_id'}\n",
    "    filters = {\n",
    "        'site_ids',\n",
    "        'message_text',  # Message text contain ...\n",
    "        'min_level',     # inclusive\n",
    "        'max_level',     # exclusive\n",
    "        'user_ids',\n",
    "        'user_agents',\n",
    "        'categories',\n",
    "        'exclude_categories',\n",
    "        'time_lost_types',\n",
    "        'exclude_time_lost_types',\n",
    "        'tags',          # at least one must be present.\n",
    "        'exclude_tags',  # all must be absent\n",
    "        'systems',\n",
    "        'exclude_systems',\n",
    "        'subsystems',\n",
    "        'exclude_subsystems',\n",
    "        'cscs',\n",
    "        'exclude_cscs',\n",
    "        'components',\n",
    "        'exclude_components',\n",
    "        'primary_software_components',\n",
    "        'exclude_primary_software_components',\n",
    "        'primary_hardware_components',\n",
    "        'exclude_primary_hardware_components',\n",
    "        'urls',\n",
    "        'min_time_lost',\n",
    "        'max_time_lost',\n",
    "        'has_date_begin',\n",
    "        'min_date_begin',\n",
    "        'max_date_begin',\n",
    "        'has_date_end',\n",
    "        'min_date_end',\n",
    "        'max_date_end',\n",
    "        'is_human',      # Allowed: either, true, false; Default=either\n",
    "        'is_valid',      # Allowed: either, true, false; Default=true\n",
    "        'min_date_added', # inclusive, TAI ISO string, no TZ\n",
    "        'max_date_added', # exclusive, TAI ISO string, no TZ\n",
    "        'has_date_invalidated',\n",
    "        'min_date_invalidated',\n",
    "        'max_date_invalidated',\n",
    "        'has_parent_id',\n",
    "        'order_by',\n",
    "        'offset',\n",
    "        'limit'\n",
    "    }\n",
    "\n",
    "    def get_messages(self,\n",
    "                     site_ids=None,\n",
    "                     message_text=None,\n",
    "                     min_date_end=None,\n",
    "                     max_date_end=None,\n",
    "                     is_human='either',\n",
    "                     is_valid='either',\n",
    "                     offset=None,\n",
    "                     limit=None,\n",
    "                     outfields=None,\n",
    "                     ):\n",
    "        qparams = dict(is_human=is_human, is_valid=is_valid)\n",
    "        if site_ids:\n",
    "            qparams['site_ids'] = site_ids\n",
    "        if message_text:\n",
    "            qparams['message_text'] = message_text\n",
    "        if min_date_end:\n",
    "            qparams['min_date_end'] = min_date_end\n",
    "        if max_date_end:\n",
    "            qparams['max_date_end'] = max_date_end\n",
    "        if limit:\n",
    "            qparams['limit'] = limit\n",
    "\n",
    "        qstr = urlencode(qparams)\n",
    "        url = f'{self.server}/{self.service}/messages?{qstr}'\n",
    "        try:\n",
    "            recs = requests.get(url, timeout=self.timeout).json()\n",
    "            recs.sort(key=lambda r: r['date_begin'])\n",
    "        except Exception as err:\n",
    "            warn(f'No {self.service} records retrieved: {err}')\n",
    "            recs = []\n",
    "\n",
    "        keep_fields(outfields, recs)\n",
    "        self.recs = recs\n",
    "        return self.recs\n",
    "\n",
    "    def get_timelost(self, rollup='day'):\n",
    "        day_tl = dict() # day_tl[day] = totalDayTimeLost\n",
    "        for day,dayrecs in itertools.groupby(\n",
    "                self.recs,\n",
    "                key=lambda r: datetime.fromisoformat(r['date_begin']).date().isoformat()\n",
    "                ):\n",
    "            day_tl[day] = sum([r['time_lost'] for r in dayrecs])\n",
    "        return day_tl\n",
    "\n",
    "class ExposurelogAdapter(SourceAdapter):\n",
    "    \"\"\"TODO full documentation\n",
    "\n",
    "    EXAMPLES:\n",
    "       gaps,recs = logrep_utils.ExposurelogAdapter(server_url='https://usdf-rsp-dev.slac.stanford.edu').get_observation_gaps('LSSTComCam')\n",
    "       gaps,recs = logrep_utils.ExposurelogAdapter(server_url='[[https://tucson-teststand.lsst.codes').get_observation_gaps('LSSTComCam')\n",
    "    \"\"\"\n",
    "    ignore_fields = ['id']\n",
    "    service = 'exposurelog'\n",
    "    endpoints = [\n",
    "        'instruments',\n",
    "        'exposures',\n",
    "        'messages',\n",
    "    ]\n",
    "    primary_endpoint = 'messages'\n",
    "    fields = {'date_added',\n",
    "              'date_invalidated',\n",
    "              'day_obs',\n",
    "              'exposure_flag',\n",
    "              'id',\n",
    "              'instrument',\n",
    "              'is_human',\n",
    "              'is_valid',\n",
    "              'level',\n",
    "              'message_text',\n",
    "              'obs_id',\n",
    "              'parent_id',\n",
    "              'seq_num',\n",
    "              'site_id',\n",
    "              'tags',\n",
    "              'urls',\n",
    "              'user_agent',\n",
    "              'user_id'}\n",
    "    filters = {\n",
    "        'site_ids',\n",
    "        'obs_id',\n",
    "        'instruments',\n",
    "        'min_day_obs',  # inclusive, integer in form YYYMMDD\n",
    "        'max_day_obs',  # exclusive, integer in form YYYMMDD\n",
    "        'min_seq_num',\n",
    "        'max_seq_num',\n",
    "        'message_text',  # Message text contain ...\n",
    "        'min_level',     # inclusive\n",
    "        'max_level',     # exclusive\n",
    "        'tags',          # at least one must be present.\n",
    "        'urls',\n",
    "        'exclude_tags',  # all must be absent\n",
    "        'user_ids',\n",
    "        'user_agents',\n",
    "        'is_human',      # Allowed: either, true, false; Default=either\n",
    "        'is_valid',      # Allowed: either, true, false; Default=true\n",
    "        'exposure_flags',\n",
    "        'min_date_added', # inclusive, TAI ISO string, no TZ\n",
    "        'max_date_added', # exclusive, TAI ISO string, no TZ\n",
    "        'has_date_invalidated',\n",
    "        'min_date_invalidated',\n",
    "        'max_date_invalidated',\n",
    "        'has_parent_id',\n",
    "        'order_by',\n",
    "        'offset',\n",
    "        'limit'\n",
    "        }\n",
    "\n",
    "\n",
    "    def check_endpoints(self, timeout=None):\n",
    "        to = (timeout or self.timeout)\n",
    "        print(f'Try connect to each endpoint of {self.server}/{self.service} '\n",
    "              f'using timeout={to}.')\n",
    "        url_http_status_code = dict()\n",
    "\n",
    "        for ep in self.endpoints:\n",
    "            qstr = '?instrument=na' if ep == 'exposures' else ''\n",
    "            url = f'{self.server}/{self.service}/{ep}{qstr}'\n",
    "            try:\n",
    "                r = requests.get(url, timeout=to)\n",
    "            except:\n",
    "                url_http_status_code[url] = 'timeout'\n",
    "            else:\n",
    "                url_http_status_code[url] = r.status_code\n",
    "        return url_http_status_code\n",
    "\n",
    "\n",
    "    def get_instruments(self):\n",
    "        url = f'{self.server}/{self.service}/instruments'\n",
    "        try:\n",
    "            instruments = requests.get(url, timeout=self.timeout).json()\n",
    "        except Exception as err:\n",
    "            warn(f'No instruments retrieved: {err}')\n",
    "            instruments = dict(dummy=[])\n",
    "        # Flatten the lists\n",
    "        return list(itertools.chain.from_iterable(instruments.values()))\n",
    "\n",
    "    def get_exposures(self, instrument, registry=1):\n",
    "        qparams = dict(instrument=instrument, registery=registry)\n",
    "        url = f'{self.server}/{self.service}/exposures?{urlencode(qparams)}'\n",
    "        try:\n",
    "            recs = requests.get(url, timeout=self.timeout).json()\n",
    "        except Exception as err:\n",
    "            warn(f'No exposures retrieved: {err}')\n",
    "            recs = []\n",
    "        return recs\n",
    "\n",
    "    def get_messages(self,\n",
    "                     site_ids=None,\n",
    "                     obs_ids=None,\n",
    "                     instruments=None,\n",
    "                     message_text=None,\n",
    "                     min_day_obs=None,\n",
    "                     max_day_obs=None,\n",
    "                     is_human='either',\n",
    "                     is_valid='either',\n",
    "                     exposure_flags=None,\n",
    "                     offset=None,\n",
    "                     limit=None,\n",
    "                     outfields=None,\n",
    "                     ):\n",
    "        qparams = dict(is_human=is_human, is_valid=is_valid)\n",
    "        if site_ids:\n",
    "            qparams['site_ids'] = site_ids\n",
    "        if obs_ids:\n",
    "            qparams['obs_ids'] = obs_ids\n",
    "        if instruments:\n",
    "            qparams['instruments'] = instruments\n",
    "        if min_day_obs:\n",
    "            qparams['min_day_obs'] = min_day_obs\n",
    "        if max_day_obs:\n",
    "           qparams['max_day_obs'] = max_day_obs\n",
    "        if exposure_flags:\n",
    "            qparams['exposure_flags'] = exposure_flags\n",
    "        if offset:\n",
    "            qparams['offset'] = offset\n",
    "        if limit:\n",
    "            qparams['limit'] = limit\n",
    "\n",
    "        qstr = urlencode(qparams)\n",
    "        url = f'{self.server}/{self.service}/messages?{qstr}'\n",
    "        recs = []\n",
    "        try:\n",
    "            response = requests.get(url, timeout=self.timeout)\n",
    "            recs = response.json()\n",
    "        except Exception as err:\n",
    "            warnings.warn(f'No {self.service} records retrieved: {err}')\n",
    "\n",
    "        if len(recs) == 0:\n",
    "            warn(f'No records retrieved from {url}')\n",
    "\n",
    "        if recs:\n",
    "            recs.sort(key=lambda r: r['day_obs'])\n",
    "\n",
    "        keep_fields(outfields, recs)\n",
    "        self.recs = recs\n",
    "        return self.recs\n",
    "\n",
    "    def get_observation_gaps(self, instruments=None,\n",
    "                             min_day_obs=None,  # YYYYMMDD\n",
    "                             max_day_obs=None,  # YYYYMMDD\n",
    "                             ):\n",
    "        if not instruments:\n",
    "            instruments = self.get_instruments()\n",
    "        assert isinstance(instruments,list), \\\n",
    "            f'\"instruments\" must be a list.  Got {instruments!r}'\n",
    "        # inst_day_rollupol[instrument] => dict[day] => exposureGapInMinutes\n",
    "        inst_day_rollup = defaultdict(dict)  # Instrument/Day rollup\n",
    "\n",
    "        for instrum in instruments:\n",
    "            recs = self.get_exposures(instrum)\n",
    "            instrum_gaps = dict()\n",
    "            for day,dayrecs in itertools.groupby(recs,\n",
    "                                                 key=lambda r: r['day_obs']):\n",
    "                gaps = list()\n",
    "                begin = end = None\n",
    "                for rec in dayrecs:\n",
    "                    begin = rec['timespan_begin']\n",
    "                    if end:\n",
    "                        # span in minutes\n",
    "                        diff = (datetime.fromisoformat(begin)\n",
    "                                - datetime.fromisoformat(end)\n",
    "                                ).total_seconds() / 60.0\n",
    "\n",
    "                        gaps.append((\n",
    "                            datetime.fromisoformat(end).time().isoformat(),\n",
    "                            datetime.fromisoformat(begin).time().isoformat(),\n",
    "                            diff\n",
    "                        ))\n",
    "                    end = rec['timespan_end']\n",
    "                instrum_gaps[day] = gaps\n",
    "\n",
    "                #!roll = dict()\n",
    "                # Rollup gap times by day\n",
    "                for day,tuples in instrum_gaps.items():\n",
    "                    #!roll[day] = sum([t[2] for t in tuples])\n",
    "                    inst_day_rollup[instrum][day] = sum([t[2] for t in tuples])\n",
    "\n",
    "        return inst_day_rollup\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Dashboard:  # TODO Complete and move to its own file.\n",
    "    \"\"\"Verify that we can get to all the API endpoints and databases we need for\n",
    "    any of our sources.\n",
    "    \"\"\"\n",
    "\n",
    "    envs = dict(\n",
    "        summit = 'https://summit-lsp.lsst.codes',\n",
    "        usdf_dev = 'https://usdf-rsp-dev.slac.stanford.edu',\n",
    "        tucson = 'https://tucson-teststand.lsst.codes',\n",
    "        # Environments not currently used:\n",
    "        #    rubin_usdf_dev = '',\n",
    "        #    data_lsst_cloud = '',\n",
    "        #    usdf = '',\n",
    "        #    base_data_facility = '',\n",
    "        #    rubin_idf_int = '',\n",
    "    )\n",
    "    adapters = [ExposurelogAdapter,\n",
    "                NarrativelogAdapter,\n",
    "                # NightReportAdapter,   # TODO\n",
    "                ]\n",
    "\n",
    "    def report(self, timeout=None):\n",
    "        url_status = dict()\n",
    "        for env,server in self.envs.items():\n",
    "            for adapter in self.adapters:\n",
    "                service = adapter(server_url=server)\n",
    "                # url_status[endpoint_url] = http_status_code\n",
    "                url_status.update(service.check_endpoints(timeout=timeout))\n",
    "        total_cnt = 0\n",
    "        good_cnt = 0\n",
    "        good = list()\n",
    "        print('\\nStatus for each endpoint URL:')\n",
    "        for url,stat in url_status.items():\n",
    "            print(f'{stat}\\t{url}')\n",
    "            total_cnt += 1\n",
    "            if stat == 200:\n",
    "                good_cnt += 1\n",
    "                good.append(url)\n",
    "        print(f'\\nConnected to {good_cnt} out of {total_cnt} endpoints.')\n",
    "        return good_cnt, good\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Exposure Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure_adapter = ExposurelogAdapter(server_url=server)\n",
    "exposure_url = exposure_adapter.source_url\n",
    "try:\n",
    "    exposure_recs = exposure_adapter.get_messages(limit=limit,\n",
    "                                        min_day_obs=min_day_obs,\n",
    "                                        max_day_obs=max_day_obs,\n",
    "                                       )\n",
    "except Exception as err:\n",
    "    exposure_recs = []\n",
    "    msg = f'ERROR getting records from {exposure_url=}: {err=}'\n",
    "    raise Exception(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Retrieved {len(exposure_recs)} records from {exposure_url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if exposure_recs:\n",
    "    new_column_names = dict(message_text='message',\n",
    "                            date_added='date'\n",
    "                        )\n",
    "    df = pd.DataFrame(exposure_recs).rename(columns=new_column_names)\n",
    "    user_df = df[['date','message']]\n",
    "    \n",
    "    display_markdown(f'### Exposure log for {number_of_days} days {min_day_obs} to {max_day_obs}', raw=True)\n",
    "    display(user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"### <font color='red'>{exposure_url}/exposures/ Not yet functional on USDF</font>\"))\n",
    "gaps = exposure_adapter.get_observation_gaps()\n",
    "if gaps:\n",
    "    for instrument, day_gaps in gaps.items():\n",
    "        display(Markdown(f'### Date vs Observation Gap (minutes) for {instrument=!s}'))\n",
    "        x,y = zip(*day_gaps.items())\n",
    "        df = pd.DataFrame(dict(day=x,minutes=y))\n",
    "        df.plot.bar(x='day', y='minutes', title=f'{instrument=!s}')\n",
    "else:\n",
    "    print(f'No Observation Gaps found in exposures.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# Narrative Log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "narrative_adapter = NarrativelogAdapter(server_url=server)\n",
    "narrative_url = narrative_adapter.source_url\n",
    "try:\n",
    "    # date like '2000-01-02 12:00:00'\n",
    "    # str(datetime(2000, 1, 2, 12, 0, 0))\n",
    "    min_date = str(datetime.strptime(min_day_obs,'%Y%m%d'))\n",
    "    max_date = str(datetime.strptime(max_day_obs,'%Y%m%d'))\n",
    "    print(f'Get data from {narrative_url}: {min_date} to {max_date}')\n",
    "    narrative_recs = narrative_adapter.get_messages(\n",
    "        limit=limit,\n",
    "        min_date_end=min_date,\n",
    "        max_date_end=max_date\n",
    "    )\n",
    "except Exception as err:\n",
    "    narrative_recs = []\n",
    "    msg = f'ERROR getting records from {narrative_url}: {err=}'\n",
    "    raise Exception(msg)\n",
    "\n",
    "print(f'Retrieved {len(narrative_recs)} records.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if narrative_recs:\n",
    "    keep_fields(['message_text','date_added'], narrative_recs)\n",
    "    new_column_names = dict(message_text='message', date_added='date')\n",
    "    df = pd.DataFrame(narrative_recs).rename(columns=new_column_names)\n",
    "    user_df = df  # [['date','message']]\n",
    "\n",
    "    display(Markdown(f'## Narrative log (Style A) for {number_of_days} days {min_day_obs} to {max_day_obs}'))\n",
    "    display(Markdown(\"### <font color='red'>Choose display Style (or offer other suggestion)</font>\"))\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        display(user_df)\n",
    "        \n",
    "    display(Markdown(f'## Narrative log (Style B)'))\n",
    "    for index,row in user_df.iterrows():\n",
    "        print(f\"{datetime.fromisoformat(user_df.iloc[0]['date']).date()}: \"\n",
    "              f\"{row['message']}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test-1')\n",
    "print('test-2')\n",
    "return\n",
    "print('test-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test-a')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
